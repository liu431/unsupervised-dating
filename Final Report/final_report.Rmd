---
title: "Final Report"
author: "Li Liu, Abhishek Pandit, Adam Shelton"
date: "12/7/2019"
header-includes:
    - \usepackage{setspace}\doublespacing
output: 
  pdf_document:
    toc: true
    toc_depth: 1
bibliography: sources.bib
---

```{r main-setup, include=FALSE}
library(tidyverse)
library(here)
library(knitr)
library(kableExtra)
library(skimr)
library(ggcorrplot)
library(treemapify)
library(factoextra)



#set working directory
#setwd('C:/Users/lliu9/Desktop/UML_Project/unsupervised-dating/Final Report')

knitr::opts_chunk$set(echo = FALSE, warning=FALSE, error = FALSE, message = FALSE , dpi = 400, tidy.opts=list(width.cutoff=50), tidy=TRUE)

# converts a Jupyter notebook to a regular old Python script
flatten_jupyter = function(path_to_notebook) {
  output_path = str_replace(path_to_notebook, ".ipynb", ".py")
  system(paste("python3", "-m", "nbconvert", "--to", "script", paste0("\"", path_to_notebook, "\"")))
  return(output_path)
}
```
# Contributions
```{r contrib, echo=FALSE}
tibble(Liu = c("Item 1", "Item 2", "Item 3"), Pandit = c("Item 1", "Item 2", "Item 3"), Shelton = c("EDA", "AGNES", "DBSCAN")) %>% kable() %>% kable_styling() %>% row_spec(0, bold = TRUE)
```

\newpage
# Introduction

“So tell me about yourself!’’ This seemingly straightforward question in day-to-day interactions is usually met with silence and hesitation. That can no longer be the case for the 1.67 trillion online dating industry, which has grown exponentially in popularity over the last decade. The dating apps, such as OkCupid and Coffee Meets Bagel, are designed to help the singles ‘get to know’ other people for short or long-term romantic relationships. In order to be popular and memorable, users usually have to write a short introduction to advertise themselves. Such activity could be regarded as self-marketing. As the users of dating apps come from diverse backgrounds, we are interested in how users from distinct backgrounds take different approaches to make themselves more memorable. Moreover, we design the framework of scoring users’ self-introduction and algorithm for providing writing tips (such as words for being memorable). Although our project is still preliminary, it has gained a lot of interest among our friends who struggle to find a date online. Also, our methods and analysis have the potential to be adopted by the dating website to improve the users’ experience and better achieve their mission as matchmakers. 

# Literature Review

Self-concept and self-representation have long served as grounds of debate in cognitive and positive psychology [@bruning1999cognitive] as well as social anthropology [@goffman1975presentation]. The recent spread of social networking and its specific affordances have allowed individuals to build different online ‘selves’ [@papacharissi2010networked]. One such critical scenario may be that of mate selection, which several economists and sociologists have likened this to ‘marriage marketplace’ [@hitsch2010matching]. Several online dating service providers in developed countries may facilitate the expansion of potential mates beyond the limits of even extended offline social networks @cacioppo2013marital assert that as many as one in three marriages in the United States is facilitated through these portals. @heino2010relationshopping argue that these avenues further entrench the economic dimension through an acute, implicit awareness of ‘relationshopping’. Herein, potential partners are reduced to entries in a catalog to be scrolled through. In this sense, they suggest an emerging conscientiousness of ‘marketing’, with the product being themselves, and the potential mate assuming the role of a buyer (ibid). This perception thus links the private worlds of romantic intimacy with those of mass consumption and broader perceived appeal to the opposite sex. 

Potentially, we will also use some marketing theories to understand our findings. Selling themselves and finding a mate on OkCupid is not very different from selling a product on eBay. Economists have been interested in the matching problem of demand and supply, such as @hitsch2010matching. Since we do not have data on users’ interactions, we will focus primarily on understanding how people brand themselves to stand out in a crowd. For example, brand awareness is a key metric in marketing to quantify the degree to which people recall or recognize a brand. A high level of brand awareness helps a product stand out and get chosen when consumers face many alternatives. 

This could be applied to understand online dating. Let us imagine your future mate uses the filter to narrow down the consideration sets. He/She might still face many similar choices with high matching scores to choose from. If you want to stand out from the pool, you must make yourself memorable by highlighting the uniqueness. Thus, one possible idea in this project is to explore and understand how users could increase their brand awareness and differentiate themselves in their segments

# Empirical Strategy

# Analysis & Results

## Exploratory Data Analysis

### Descriptive Statistics

```{r descr-stats-demo, echo=FALSE, cache=TRUE}
original_data = read_csv(here("Data", "final_okcupid.csv"))
skim_list = original_data %>% select(-c(dbscan_cluster, new_index, orig_index)) %>% skim() %>% partition()

skim_list$numeric %>% select(-hist) %>% mutate_if(is.numeric, round, digits = 2) %>% kable(caption = "Continuous Variables")
skim_list$character%>% kable(caption = "Other Variables")

original_data %>% select(-c(orig_index, new_index, clean_text, essay9, dbscan_cluster)) %>% select_if(is.numeric) %>% cor(use = "pairwise.complete.obs") %>% ggcorrplot() + labs(title = "Correlation Plot of Demographic Variables")
```

The data we used was approximately 60,000 anonymous OkCupid profiles from 2012 that were gathered with consent from users in the San Francisco area [@kim2015okcupid]. This data was downloaded from the GitHub page for @kim2015okcupid, [https://github.com/rudeboybert/JSE_OkCupid](https://github.com/rudeboybert/JSE_OkCupid). The data contains demographic attributes of users that were submitted to their profile, including variables like age, height, race, and education, in addition to a selection of ten short essays that users have written in response to different prompts to display on their profiles. We subsetted this data to `r nrow(original_data)` profiles of men, and generated additional features for the numbers and proportions of long words and Flesch–Kincaid readability scores of the main profile essay. The majority of male users in our sample or white, fit, not-short, and have more than a high school education. The mean reported age is 32 and the mean reported height is 70.5 inches (approximately 5 foot 9 inches).

The majority of the variables included in the demographic data are independent, but some weaker correlations do exist. As expected there are positive correlations between all the features generated from the essay text. Age is also positively correlated with our text-generated features, perhaps suggesting that older people are more educated and write with more complexity. While Flesch scores and the amount of long words are correlated, there do not appear to be any demographic interactions with that relationship.

```{r viz-analysis, echo=FALSE, cache=TRUE}
original_data %>% 
  select(edu, fit, height_group, race_ethnicity) %>% 
  mutate_all(factor) %>% 
  pivot_longer(dplyr::everything()) %>% 
  table() %>% 
  as_tibble() %>% 
  ggplot(aes(area = n, fill = value, label = value)) + 
  geom_treemap() + 
  geom_treemap_text(color = "white", place = "centre", grow = TRUE) + 
  facet_wrap(~ name) + theme(legend.position = "none") + 
  labs(title = "Categorical Variable Distributions")

original_data %>% select(-new_index, -orig_index, -age, -height, -clean_text, -essay9, -dbscan_cluster, -profile_length, -prop_longwords) %>% pivot_longer(-c(flesch, long_words)) %>% ggplot(aes(x = log(flesch + abs(min(flesch)) + 1), y = long_words, color = value)) + geom_point(alpha = 0.3, size = 1) + facet_wrap(~ name) + theme(legend.position = "none") + labs(title = "Flesch score vs. Long words by Variable", x = "Flesch Score (log)", y = "Number of Long Words")
```


### Clusterability

```{r demo-clusterability, cache=TRUE}
clusterability = original_data %>% select(-c(new_index, orig_index, clean_text, essay9, dbscan_cluster)) %>% mutate_if(is.character, factor) %>% mutate_all(as.numeric) %>% sample_n(2000) %>% scale() %>% get_clust_tendency(n = 10)
clusterability$plot
```
The demographic data is highly clusterable with a Hopkins Statistic of `r round(clusterability$hopkins, 3)` on a random subset of 2,000 observations. Unfortuantely, clusterability could not be determined for the whole dataset due to performance limitations. However, 2,000 observations should be sufficient for determining clusterability. 

## Clustering of Demographic Data

### K-means

### AGNES

Agglomerative Nesting was used to cluster the demographic data with a bottom-up approach to contrast the K-means clustering. As an AGNES model would not complete on the full data-set, a random subset of 5,000 observations was used instead. This AGNES model used a Gower's dissimilarity matrix of the data to aid in the clustering of categorical variables, since most of the demographic variables are categorical. Using the `NbClust` package in R, we determined that the optimal number of clusters was two, when using the `ward.D2` method to match our AGNES model. The model gives us two well-defined clusters along the first two principal components. The defining factor between these two clusters, majorly appears to be height, which, per the Wilcox Test, has a statistically significant difference between the means of the two clusters by about five inches.

![NbClust Results](../Clustering/advanced_clustering_files/figure-gfm/agg-nest-3.png)

![AGNES Dendrogram](../Clustering/advanced_clustering_files/figure-gfm/agg-nest-4.png)

![AGNES Cluster Results](../Clustering/advanced_clustering_files/figure-gfm/agg-nest-5.png)

## Text Analysis

### Word2Vec

### Topic Modeling

### DBSCAN

As we wanted to determine factors behind why a profile might stick out, we decided to use 
a DBSCAN model was used to detect outliers within a data-set of 50 vectors calculated by Doc2Vec. As Doc2Vec vectors capture different characteristics about the text, any outliers in these vectors should be profiles that deviate from the norm to some degree. As shown below, these Doc2Vec vectors are highly independent with very few correlations, but also highly clusterable, with a Hopkins Statistic of 0.808.

![Doc2Vec Correlation Plot](../Clustering/dbscan_doc2vec_files/figure-gfm/data-1.png)

Using a K-nearest-neighbors distribution plot, we determined that the optimal value for the epsilon neighborhood size of the DBSCAN model was 9. This was determined using 5 nearest neighbors, to match the default minimum number of points in the epsilon region, which we used for the model.

![DBSCAN KNN Distribution Plot](../Clustering/dbscan_doc2vec_files/figure-gfm/dbscan-1.png)

The DBSCAN model identified one cluster, and 108 outliers, accounting for 0.57% of the observations. Using a Wilcox Test, we can determine that the difference in the means between the "typical" profiles and outlier profiles are insignificant for the continuous variables for height and number of long words, but are highly significant for Flesch score and age. Mean age is higher among the outliers by about 4.5 years, while the mean Flesch score of outliers is about 8 points higher. This suggests that the outliers, being older and writing at a more advanced level, might be more educated (or at least trying to appear so) than their "typical" peers.

![DBSCAN Cluster Plot](../Clustering/dbscan_doc2vec_files/figure-gfm/dbscan-2.png)

## Combining Text and Demographic Data


## Output

<img width="30%" src="preference_selection.png"/>
<img width="30%" src="profile_help.png"/>
<img width="30%" src="profile_help_fixed.png"/>

The above three pictures show the protocol of the final product that we have in mind. The protocol uses the example of Li (one of the project members), although the information is hypothetical. The first picture displays his demographic information and a short self-introduction. The second picture shows that the new algorithm highlights the common and memorable words by comparing them with the frequencies among his peers. He also gets a score of 63 and some tips on improving the writing. The third picture is the revised version of previous writing, which now Li has a memorable self-introduction with an increased score of 87. He should be confident to reach out to other users on OkCupid now. 


## Discussion

The project faces several limitations at this stage. Firstly, without data on the outcome (such as the number of messages), we do not know have a relative measure for the self-introductions' memorabilities. As a result, we don't have the outcomes to train the scoring function with machine learning. Secondly, without follow-up interviews, we cannot measure whether the specific choice of words was aimed at authenticity or matches with an awareness of 'relationshopping' (experienced users use certain phrases fraudulently to hook others). Thirdly, without the users' other profile images, we cannot estimate the effect of a memorable self-introduction on outcomes as the quality of images is a potential moderating variable.

# Conclusion

\newpage
# Appendices

## AGNES Code

```{r code = readLines(knitr::purl(here("Clustering", "advanced_clustering.Rmd"), documentation = 1)), echo = T, eval = F}

```

## DBSCAN Code

```{r code = readLines(knitr::purl(here("Clustering", "dbscan_doc2vec.Rmd"), documentation = 1)), echo = T, eval = F}

```

## Doc2Vec Code

```{python code = readLines(flatten_jupyter(here("Vector_Space_Model", "Doc2Vec_Modelling.ipynb"))), echo = T, eval = F}

```

\newpage
# References
